{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "seq-seq model self-Attention\n",
    "RNN :\n",
    "Recurrent neural  network\n",
    "slot filling 給定句子  機器判定哪兩個是slot\n",
    "\n",
    "詞彙向量 one-of-n encoder\n",
    "\n",
    "\n",
    "GAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from transformers) (2.25.0)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp38-cp38-macosx_10_11_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp38-cp38-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.56.0-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 3.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->transformers) (1.26.2)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 3.6 MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=8f06769f4624cd2f104601bf046c2030af8caf38739b4afb4aed8b1b30df852b\n",
      "  Stored in directory: /Users/jeremy/Library/Caches/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tqdm, regex, click, tokenizers, sacremoses, filelock, transformers\n",
      "Successfully installed click-7.1.2 filelock-3.0.12 regex-2020.11.13 sacremoses-0.0.43 tokenizers-0.9.4 tqdm-4.56.0 transformers-4.2.2\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import(\n",
    "BertTokenizerFast,\n",
    "AutoModelForMaskedLM,\n",
    "AutoModelForCausalLM,\n",
    "AutoModelForTokenClassification,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884d6bce4db64f09b21565205588f129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d3bb6c274b429a858173c7219707a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#masked language model(GPT2)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForMaskedLM.from_pretrained('ckiplab/albert-tiny-chinese')#or other models above\n",
    "#casual language model(GPT2)\n",
    "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "#model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese')\n",
    "#nlp task model\n",
    "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "#model = AutoModelForTokenClassification.from_pretrained('ckiplab/albert-tiny-chinese-ws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import module\n",
    "from ckip_transformers.nlp import CkipWordSegmenter,CkipPosTagger,CkipNerChunker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfaa2826eebb48bb9584b827e96efe13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/804 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6024aca582445699b509ef89d10ec69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/407M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a35884ee16640059cd013fd32835524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5816e8c5bb943a2ae58096834f112d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648a3ada693f41f9b2595e500acdfb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432a11ae45a64c37a1739bffc108bf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2b4524876048ab98bc90a2ec541f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/407M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a818a9f09188454e9f87c5de690e46af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d114113eaf54ac2957633dbb5c3b18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09f5ddb6eb1487db92011b2240160cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b2f71a019b4364b7a28850cd79c7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228bce7ab7774d7d8dcf783eb9003985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/407M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0582d6d6df64418a7e162552dcfbdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3bdd313e9845b088dd367d499a8d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad707ffbaea426fb9fc031e2c26c8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#initialize drivers\n",
    "ws_driver = CkipWordSegmenter(level=3)\n",
    "pos_driver = CkipPosTagger(level=3)\n",
    "ner_driver = CkipNerChunker(level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#可於宣告斷詞等工具時指定device 以使用GPU，設為-1，代表\n",
    "不使用GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use GPU,one may specify device ID while initialize the drivers.Set to -1(default) to disable GPU.\n",
    "ws_driver = CkipWordSegmenter(device=-1)\n",
    "#ws_driver = CkipWordSegmenter(device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline\n",
    "\n",
    "###  斷詞與實體辨識的輸入必須是list  of sentences.\n",
    "### 詞性標記的輸入必須是list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = [\n",
    "   '傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。',\n",
    "   '美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。',\n",
    "   '空白 也是可以的～',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。',\n",
       " '美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。',\n",
       " '空白 也是可以的～']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "斷詞、實體辨識輸入必須是list of sentences。\n",
    "詞性標記的輸入必須是list of list of words。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 2510.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 2379.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 3386.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "ws  = ws_driver(text)\n",
    "pos = pos_driver(ws)\n",
    "ner = ner_driver(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞性標記工具會自動用', 。 : ; ! ?' 等字元在執行模型前切割句(輸出的句子會自動接回)。\n",
    "可設定 delim_set 參數使用別的字元做切割。\n",
    "另外可指定 use_delim=False 來已停用此功能，或於斷詞、實體辨識時指定use_delim=False 已啟用此功能。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 1713.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 11194.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "#Enable sentence segmentation\n",
    "ws = ws_driver(text,use_delim=True)\n",
    "ner = ner_driver(text,use_delim=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 1584.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "#Disable sentence segmentation\n",
    "pos = pos_driver(ws,use_delim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 4108.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "#use new line characters and tabs for sentence segmentation\n",
    "pos = pos_driver(ws,delim_set='\\n\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ws 斷詞：將一個句子斷開\n",
    "\n",
    "\n",
    "pos 詞性標注：指將斷詞結果做詞性的標註，\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ner專有名詞辨識：目標為擷取文字資料中指向實體（entities）的文字區塊，例如：人名、地名、組織名，在生醫領域中也可能是藥品名、分子式等等。NER 讓機器能自動找尋文本中提到的我們感興趣的實體，例如公眾人物等，並加以分析，其產出亦作為人工智慧理解自然語言的重要資訊。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Sequence length is longer than the maximum sequence length for this model (510 > 510).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-60584a20e48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mws_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m510\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ckip_transformers/nlp/driver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_text, use_delim, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mindex_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         ) = super().__call__(input_text, use_delim=use_delim, **kwargs)\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Post-process results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ckip_transformers/nlp/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_text, use_delim, delim_set, batch_size, max_length, show_progress)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mmodel_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_max_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# Add [CLS] and [SEP]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmodel_max_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0;34m'Sequence length is longer than the maximum sequence length for this model '\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                \u001b[0;34mf'({max_length} > {model_max_length}).'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Sequence length is longer than the maximum sequence length for this model (510 > 510)."
     ]
    }
   ],
   "source": [
    "ws = ws_driver(text,batch_size=256, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/////////////text//////////////\n",
      "傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。\n",
      "--------ws&pos-----------\n",
      "傅達仁(Nb)　今(Nd)　將(D)　執行(VC)　安樂死(Na)　，(COMMACATEGORY)　卻(D)　突然(D)　爆出(VJ)　自己(Nh)　20(Neu)　年(Nd)　前(Ng)　遭(P)　緯來(Nb)　體育台(Na)　封殺(VC)　，(COMMACATEGORY)　他(Nh)　不(D)　懂(VK)　自己(Nh)　哪裡(Ncd)　得罪到(VC)　電視台(Nc)　。(PERIODCATEGORY)\n",
      "===========entity=============\n",
      "NerToken(word='傅達仁', ner='PERSON', idx=(0, 3))\n",
      "NerToken(word='20年', ner='DATE', idx=(18, 21))\n",
      "NerToken(word='緯來體育台', ner='ORG', idx=(23, 28))\n",
      "\n",
      "/////////////text//////////////\n",
      "美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。\n",
      "--------ws&pos-----------\n",
      "美國(Nc)　參議院(Nc)　針對(P)　今天(Nd)　總統(Na)　布什(Nb)　所(D)　提名(VC)　的(DE)　勞工部長(Na)　趙小蘭(Nb)　展開(VC)　認可(VC)　聽證會(Na)　，(COMMACATEGORY)　預料(VE)　她(Nh)　將(D)　會(D)　很(Dfa)　順利(VH)　通過(VC)　參議院(Nc)　支持(VC)　，(COMMACATEGORY)　成為(VG)　該(Nes)　國(Nc)　有史以來(D)　第一(Neu)　位(Nf)　的(DE)　華裔(Na)　女性(Na)　內閣(Na)　成員(Na)　。(PERIODCATEGORY)\n",
      "===========entity=============\n",
      "NerToken(word='美國參議院', ner='ORG', idx=(0, 5))\n",
      "NerToken(word='今天', ner='LOC', idx=(7, 9))\n",
      "NerToken(word='布什', ner='PERSON', idx=(11, 13))\n",
      "NerToken(word='勞工部長', ner='ORG', idx=(17, 21))\n",
      "NerToken(word='趙小蘭', ner='PERSON', idx=(21, 24))\n",
      "NerToken(word='認可聽證會', ner='EVENT', idx=(26, 31))\n",
      "NerToken(word='參議院', ner='ORG', idx=(42, 45))\n",
      "NerToken(word='第一', ner='ORDINAL', idx=(56, 58))\n",
      "NerToken(word='華裔', ner='NORP', idx=(60, 62))\n",
      "\n",
      "/////////////text//////////////\n",
      "空白 也是可以的～\n",
      "--------ws&pos-----------\n",
      "空白(VH)　 (WHITESPACE)　也(D)　是(SHI)　可以(VH)　的(T)　～(FW)\n",
      "===========entity=============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pack word segmentation and part-of-speech results:\n",
    "def pack_ws_pos_sentence(sentence_ws,sentence_pos):\n",
    "    assert len(sentence_ws) == len(sentence_pos)\n",
    "    res=[]\n",
    "    for word_ws,word_pos in zip(sentence_ws,sentence_pos):\n",
    "        res.append(f'{word_ws}({word_pos})')\n",
    "    return '\\u3000'.join(res)\n",
    "#show results\n",
    "for sentence,sentence_ws,sentence_pos,sentence_ner in zip(text,ws,pos,ner):\n",
    "    print('/////////////text//////////////')\n",
    "    print(sentence)\n",
    "    print('--------ws&pos-----------')\n",
    "    print(pack_ws_pos_sentence(sentence_ws,sentence_pos))\n",
    "    print('===========entity=============')\n",
    "    for entity in sentence_ner:\n",
    "        print(entity)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以下示範f的操作，\n",
    "\n",
    "那麼何謂f？\n",
    "當一字串前面出現小寫f則表示該字串當中有需要被取代的項目，\n",
    "而被取代的項目用大括弧表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world\n"
     ]
    }
   ],
   "source": [
    "#text = 'world'\n",
    "print(f'Hello, {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y = 37\n"
     ]
    }
   ],
   "source": [
    "x = 10\n",
    "y = 27\n",
    "\n",
    "print(f'x + y = {x + y}')\n",
    "# 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
